{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.inferences import VariationalInference\n",
    "from edward.models import Bernoulli, Normal, RandomVariable\n",
    "from edward.util import copy\n",
    "from scipy.special import expit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (16.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-167975fffe91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_zoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Code/bayesian-forecast/src/model_zoo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# use custom helper functions from src folder\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "from data import *\n",
    "from inference import *\n",
    "from helpers import *\n",
    "from model_zoo import conv_lstm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this basic example, lets only examine a small subset of the data\n",
    "%pdb on\n",
    "N = 1000  # number of data points\n",
    "D = 5  # number of features\n",
    "X_data, y_data, X_test, y_test = get_dataset(N=N,D=D, make_balanced=True)\n",
    "print('shapes (X,y),', X_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_data)\n",
    "df['target'] = y_data\n",
    "sns.heatmap(df.corr());\n",
    "# title='correlation of features and binary target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_data[:,0], y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 128    # batch size during training\n",
    "y_ph = tf.placeholder(tf.int32, [None])\n",
    "data = generator([X_data, y_data], M)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = conv_lstm(features=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None , D])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Bernoulli(logits=(ed.dot(X, w)+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qw = Normal(loc=tf.Variable(tf.random_normal([D])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "qb = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to maximize the Evidence Lower bound (moved \"log\" outside compared to standard VAE)\n",
    "\n",
    "$$ log(p(x)) >= -E_{q(z^1; \\lambda), ..., q(z^K; \\lambda)} [ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p(x, z^k)}{q(z^k; \\lambda)} ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inference = IWVI({w: qw,  b: qb}, data={X: X_data, y: y_data})\n",
    "# inference.run(K=5, n_iter=10000, n_print=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we use KLqp for batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X_data)\n",
    "n_batch = int(N / M)\n",
    "n_epoch = 100\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={y: y_ph})\n",
    "inference.initialize(n_iter=n_batch * n_epoch, n_samples=5, scale={y: N / M})\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "info_dicts = []\n",
    "for _ in range(inference.n_iter):\n",
    "  X_batch, y_batch = next(data)\n",
    "  info_dict = inference.update({X: X_batch, y_ph: y_batch})\n",
    "  inference.print_progress(info_dict)\n",
    "  info_dicts.append(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRITICISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {w: qw, b:qb})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### todo: implemention of edward's logloss metric is buggy as it accepts predictions instead of logits\n",
    "#### I've raised an issue here: https://github.com/blei-lab/edward/issues/795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sam,Xx,yy in [(\"INS\", X_data, y_data), (\"OOS\",X_test, y_test)]:\n",
    "    print(sam)\n",
    "    print(\"binary_acc:\", ed.evaluate('binary_accuracy', data={X: Xx, y_post: yy}))\n",
    "    print(\"binary_cross_entropy:\", ed.evaluate('log_loss', data={X: Xx, y_post: yy}))  # todo bug in implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"log_loss:\", ed.evaluate('log_loss', data={X: X_data, y_post: y_data})) # todo bug in implementation\n",
    "print(\"categorical_crossentropy:\", ed.evaluate('categorical_crossentropy', data={X: X_data, y_post: y_data}))\n",
    "print(\"log_likelihood:\", ed.evaluate('log_likelihood', data={X: X_data, y_post: y_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = visualise(X_data, y_data, w, b) #prior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = visualise(X_data, y_data, qw, qb) #posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true, y_pred):\n",
    "  \"\"\"Binary cross-entropy.\n",
    "  Args:\n",
    "    y_ue: tf.Tensor.\n",
    "      Tensor of 0s and 1s.\n",
    "    y_pred: tf.Tensor.\n",
    "      Tensor of real values (logit probabilities), with same shape as\n",
    "      `y_true`.\n",
    "  \"\"\"\n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  y_pred = tf.cast(y_pred, tf.float32)\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.util import check_data, get_session\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def get_y_preds(X_data, y_data, w, b, n_samples=10):\n",
    "    w_samples = [w.eval() for _ in range(n_samples)]\n",
    "    b_samples = [b.eval() for _ in range(n_samples)]\n",
    "    return w_samples, b_samples\n",
    "\n",
    "def eval_model(Xx, Yy, nom):\n",
    "    ans = get_y_preds(Xx, Yy, qw, qb, n_samples=10000)\n",
    "    zw, zb = np.mean(ans[0], axis=0), np.mean(ans[1], axis=0)\n",
    "    probabilities = sigmoid(Xx.dot(zw) + zb)\n",
    "    #probabilities = probabilities.clip(.49,.51)\n",
    "    ins = pd.DataFrame(probabilities)\n",
    "    ins['target'] = Yy\n",
    "    ins['guess'] = ins[0].round().clip(0,1)\n",
    "    print(nom, '\\n------')\n",
    "    print('acc:', (ins['target'] == ins['guess']).mean())\n",
    "    print('sk.logloss:', log_loss(ins['target'], ins[0]))\n",
    "    sess = get_session()\n",
    "    print('ed.logloss (corrected):', sess.run(binary_crossentropy(ins['target'], -np.log(1/ins[0] -1)   )   ))\n",
    "    print('ed.logloss (current):', sess.run(binary_crossentropy(ins['target'], ins[0]   )   ))\n",
    "\n",
    "    print ('')\n",
    "    return ins\n",
    "\n",
    "ins = eval_model(X_data, y_data, 'ins')\n",
    "oos = eval_model(X_test, y_test, 'oos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how eval metrics look under sample of maximum uncertainy\n",
    "\n",
    "from edward.util import check_data, get_session\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "preds = np.array([.5]*1000)\n",
    "target = np.random.randint(0,2,1000)\n",
    "\n",
    "print('acc:', (target == preds.round()).mean())\n",
    "print('logloss:', log_loss(target, preds))\n",
    "sess = get_session()\n",
    "print('ed.logloss:', sess.run(binary_crossentropy(target, preds)))\n",
    "\n",
    "x = preds\n",
    "z = target\n",
    "print('tf.doc.logloss', np.mean(x - x * z + np.log(1 + np.exp(-abs(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ins[0].plot(kind='hist',subplots=True,sharex=False,sharey=True,title='distribution of predictions vs target', bins=50)\n",
    "ax[0].axvline(.5, color='k', linestyle='--')\n",
    "#ins['target'].plot(kind='hist', ax=ax[0], color='red', bins=50)\n",
    "\n",
    "(((ins[0]-.5)*.25)+.5).plot(kind='hist', ax=ax[0], color='green', bins=50) # strinkage around 50%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_post = ed.copy(X, {w: qw, b: qb})\n",
    "y_rep, y = ed.ppc(\n",
    "    lambda xs, zs: tf.reduce_mean(tf.cast(xs[x_post], tf.float32)),\n",
    "    data={x_post: X_data})\n",
    "\n",
    "ed.ppc_stat_hist_plot(\n",
    "    y[0], y_rep, stat_name=r'$T \\equiv$mean', bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.ppc(lambda xs, zs: tf.reduce_mean(tf.cast(xs[x_post], tf.float32)), data={x_post: X_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T(xs, zs):\n",
    "    return tf.reduce_max(xs[y_post])\n",
    "\n",
    "ppc_max = ed.ppc(T, data={X: X_data, y_post: y_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb.sample(50000).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## correlation with target is approxmently mean of posterior distribution on parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X_data[:,0])\n",
    "df['target'] = y_data\n",
    "sns.heatmap(df.corr(), annot=True, ax=axs[0], annot_kws={'size':22})\n",
    "dc = pd.Series(qw.sample(50000).eval()[:,0])\n",
    "ax = dc.plot(kind='hist',subplots=True,sharex=False,sharey=True,title='distribution of qw_0', bins=50, ax=axs[1])\n",
    "\n",
    "dcsummary = pd.DataFrame(dc.describe())\n",
    "plt.table(cellText=dcsummary.values,colWidths = [0.25]*len(dcsummary.columns),\n",
    "          rowLabels=dcsummary.index,\n",
    "          colLabels=dcsummary.columns,\n",
    "          cellLoc = 'center', rowLoc = 'center',\n",
    "          loc='top');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ppc is 1 since y_data.mean() > 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under toy example, bayesian method looks poor\n",
    "\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_data, y_data)\n",
    "\n",
    "print('baseline')\n",
    "print('--------')\n",
    "print('ins:', (model.predict_proba(X_data).round()[:,1] == y_data).mean())\n",
    "print('oos:', (model.predict_proba(X_test).round()[:,1] == y_test).mean())\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print('bayesian-forecast')\n",
    "print('--------')\n",
    "ins = eval_model(X_data, y_data, 'ins')\n",
    "oos = eval_model(X_test, y_test, 'oos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
